{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Say hi to LightDiffusion-Next \ud83d\udc4b","text":"<p>LightDiffusion-Next  is the fastest AI-powered image generation GUI/CLI, combining speed, precision, and flexibility in one cohesive tool.</p> <p>As a refactored and improved version of the original LightDiffusion repository, this project enhances usability, maintainability, and functionality while introducing a host of new features to streamline your creative workflows.</p>"},{"location":"#motivation","title":"Motivation:","text":"<p>LightDiffusion was originally meant to be made in Rust, but due to the lack of support for the Rust language in the AI community, it was made in Python with the goal of being the simplest and fastest AI image generation tool.</p> <p>That\u2019s when the first version of LightDiffusion was born which only counted 3000 lines of code, only using Pytorch. With time, the project grew and became more complex, and the need for a refactor was evident. This is where LightDiffusion-Next comes in, with a more modular and maintainable codebase, and a plethora of new features and optimizations.</p> <p>\ud83d\udcda Learn more in the official documentation.</p>"},{"location":"#highlights","title":"\ud83c\udf1f Highlights","text":"<p>LightDiffusion-Next offers a powerful suite of tools to cater to creators at every level. At its core, it supports Text-to-Image (Txt2Img) and Image-to-Image (Img2Img) generation, offering a variety of upscale methods and samplers, to make it easier to create stunning images with minimal effort.</p> <p>Advanced users can take advantage of features like attention syntax, Hires-Fix or ADetailer. These tools provide better quality and flexibility for generating complex and high-resolution outputs.</p> <p>LightDiffusion-Next is fine-tuned for performance. Features such as Xformers acceleration, BFloat16 precision support, WaveSpeed dynamic caching, and Stable-Fast model compilation (which offers up to a 70% speed boost) ensure smooth and efficient operation, even on demanding workloads.</p>"},{"location":"#feature-showcase","title":"\u2728 Feature Showcase","text":"<p>Here\u2019s what makes LightDiffusion-Next stand out:</p> <ul> <li> <p>Speed and Efficiency:   Enjoy industry-leading performance with built-in Xformers, Pytorch, Wavespeed and Stable-Fast optimizations, achieving up to 30% faster speeds compared to the rest of the AI image generation backends in SD1.5 and up to 2x for Flux.</p> </li> <li> <p>Automatic Detailing:   Effortlessly enhance faces and body details with AI-driven tools based on the Impact Pack.</p> </li> <li> <p>State Preservation:   Save and resume your progress with saved states, ensuring seamless transitions between sessions.</p> </li> <li> <p>Advanced GUI, WebUI and CLI:   Work through a user-friendly graphical interface as GUI or in the browser using Gradio or leverage the streamlined pipeline for CLI-based workflows.</p> </li> <li> <p>Integration-Ready:   Collaborate and create directly in Discord with Boubou, or preview images dynamically with the optional TAESD preview mode.</p> </li> <li> <p>Image Previewing:   Get a real-time preview of your generated images with TAESD, allowing for user-friendly and interactive workflows.</p> </li> <li> <p>Image Upscaling:   Enhance your images with advanced upscaling options like UltimateSDUpscaling, ensuring high-quality results every time.</p> </li> <li> <p>Prompt Refinement:     Use the Ollama-powered automatic prompt enhancer to refine your prompts and generate more accurate and detailed outputs.</p> </li> <li> <p>LoRa and Textual Inversion Embeddings:     Leverage LoRa and textual inversion embeddings for highly customized and nuanced results, adding a new dimension to your creative process.</p> </li> <li> <p>Low-End Device Support:     Run LightDiffusion-Next on low-end devices with as little as 2GB of VRAM or even no GPU, ensuring accessibility for all users.</p> </li> </ul>"},{"location":"#performance-benchmarks","title":"\u26a1 Performance Benchmarks","text":"<p>LightDiffusion-Next dominates in performance:</p> Tool Speed (it/s) LightDiffusion with Stable-Fast 2.8 LightDiffusion 1.8 ComfyUI 1.4 SDForge 1.3 SDWebUI 0.9 <p>(All benchmarks are based on a 1024x1024 resolution with a batch size of 1 using BFloat16 precision without tweaking installations. Made with a 3060 mobile GPU using SD1.5.)</p> <p>With its unmatched speed and efficiency, LightDiffusion-Next sets the benchmark for AI image generation tools.</p>"},{"location":"#installation","title":"\ud83d\udee0 Installation","text":"<p>To get started with LightDiffusion, follow the installation guide which provides detailed steps for setting up the project on your system.</p>"},{"location":"#usage","title":"Usage","text":"<p>LightDiffusion-Next offers a variety of features and customization options. Refer to the Prompting Guide and HiresFix &amp; Adetailer Guide for detailed instructions on how to use these features effectively.</p>"},{"location":"#community-and-support","title":"Community and Support","text":"<p>Join our community on GitHub to share your creations, report issues, and contribute to the project. Check out the FAQ for common questions and troubleshooting tips.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions from the community. If you\u2019re interested in contributing, please read our contributing guidelines to get started.</p>"},{"location":"#license","title":"License","text":"<p>LightDiffusion-Next is released under the GNU GPLv3. Feel free to use, modify, and distribute this software in accordance with the license terms.</p> <p>Thank you for using LightDiffusion! We hope you enjoy creating amazing visuals with our tool.</p>"},{"location":"AutoHDR/","title":"AutoHDR Guide for LightDiffusion-Next","text":"<p>Welcome to the AutoHDR guide for LightDiffusion-Next. This document will help you understand how to use the AutoHDR feature to enhance the dynamic range and visual quality of your generated images.</p>"},{"location":"AutoHDR/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>What is AutoHDR?</li> <li>Usage    - In the GUI    - In the CLI</li> <li>Visual Comparison</li> <li>Tips and Tricks</li> <li>Troubleshooting</li> <li>FAQ</li> </ol>"},{"location":"AutoHDR/#introduction","title":"Introduction","text":"<p>LightDiffusion-Next includes an advanced AutoHDR feature that automatically enhances the dynamic range of generated images, resulting in more vibrant colors, better contrast, and improved overall visual quality. This guide will explain how to use this feature effectively.</p>"},{"location":"AutoHDR/#what-is-autohdr","title":"What is AutoHDR?","text":"<p>AutoHDR (Automatic High Dynamic Range) is a post-processing technique that:</p> <ul> <li>Expands the dynamic range of your images</li> <li>Improves contrast and color saturation</li> <li>Enhances highlights and shadow details</li> <li>Makes images appear more vibrant and visually appealing</li> <li>Adds depth to flat-looking images</li> </ul> <p>The implementation in LightDiffusion-Next is designed to work seamlessly with the generation process, applying intelligent adjustments based on the content of each image.</p>"},{"location":"AutoHDR/#usage","title":"Usage","text":""},{"location":"AutoHDR/#in-the-gui","title":"In the GUI","text":"<p>In the LightDiffusion-Next graphical interface, AutoHDR is enabled by default. You don\u2019t need to take any additional steps to use this feature.</p> <p>To generate images with AutoHDR:</p> <ol> <li>Launch LightDiffusion-Next: Start the application using <code>run.bat</code> (Windows) or <code>run.sh</code> (Linux).</li> <li>Configure Settings: Set your desired parameters for image generation.</li> <li>Generate Images: Click the \u201cGenerate\u201d button to create images with AutoHDR enhancement automatically applied.</li> </ol>"},{"location":"AutoHDR/#in-the-cli","title":"In the CLI","text":"<p>When using the command-line interface, AutoHDR is available as a toggle option.</p> <p>To enable AutoHDR in the CLI:</p> <pre><code>./pipeline.bat \"your prompt here\" width height number_of_images batch_size --autohdr\n</code></pre> <p>For example:</p> <pre><code>./pipelin.bat \"a beautiful sunset over mountains\" 1024 768 1 1 --autohdr\n</code></pre>"},{"location":"AutoHDR/#visual-comparison","title":"Visual Comparison","text":"<p>To demonstrate the impact of AutoHDR, here is a visual comparison of an image generated with and without AutoHDR:</p> <p>above: Without AutoHDR - Notice the flatter colors and less defined contrast Under: With AutoHDR - Observe the improved color vibrancy, better contrast, and enhanced details</p> <p> </p>"},{"location":"CODE_OF_CONDUCT/","title":"Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#introduction","title":"Introduction","text":"<p>Welcome to the LightDiffusion community! We are committed to providing a friendly, safe, and welcoming environment for all, regardless of gender, sexual orientation, disability, ethnicity, religion, or age. This Code of Conduct outlines our expectations for participants within the community, as well as the steps to reporting unacceptable behavior.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":""},{"location":"CODE_OF_CONDUCT/#be-respectful","title":"Be Respectful","text":"<ul> <li>Treat everyone with respect. </li> <li>Be considerate of differing viewpoints and experiences.</li> <li>Gracefully accept constructive criticism.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#be-inclusive","title":"Be Inclusive","text":"<ul> <li>Seek to understand and respect different perspectives.</li> <li>Avoid discriminatory or exclusionary behavior.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#be-collaborative","title":"Be Collaborative","text":"<ul> <li>Share knowledge and help others.</li> <li>Foster a welcoming environment for collaboration.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#be-mindful","title":"Be Mindful","text":"<ul> <li>Be aware of your surroundings and of your fellow participants.</li> <li>Alert community leaders if you notice a dangerous situation or someone in distress.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#unacceptable-behavior","title":"Unacceptable Behavior","text":"<p>Examples of unacceptable behavior include:</p> <ul> <li>Harassment, intimidation, or discrimination in any form.</li> <li>Offensive comments related to gender, sexual orientation, disability, ethnicity, religion, or age.</li> <li>The use of sexualized language or imagery.</li> <li>Unwelcome sexual attention or advances.</li> <li>Trolling, insulting, or derogatory comments.</li> <li>Public or private harassment.</li> <li>Publishing others\u2019 private information without explicit permission.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#reporting-and-enforcement","title":"Reporting and Enforcement","text":""},{"location":"CODE_OF_CONDUCT/#reporting","title":"Reporting","text":"<p>If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community leader as soon as possible. You can report issues by:</p> <ul> <li>Emailing me at melis.emilio1@gmail.com</li> <li>Opening an issue on our GitHub repository</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Community leaders are responsible for clarifying and enforcing our standards. They may take appropriate and fair corrective action in response to any behavior they deem inappropriate, including:</p> <ul> <li>A warning to the offender.</li> <li>Temporary or permanent expulsion from the community without warning.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Thank you for helping to make this a welcoming, friendly, and productive community for everyone.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to LightDiffusion","text":"<p>Thank you for considering contributing to LightDiffusion! Your help is greatly appreciated. By contributing, you help improve the project and make it more useful for everyone.</p>"},{"location":"CONTRIBUTING/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>How to Contribute</li> <li>Code of Conduct</li> <li>Getting Started</li> <li>Reporting Issues</li> <li>Submitting Changes</li> <li>Style Guide</li> <li>Community and Support</li> </ol>"},{"location":"CONTRIBUTING/#introduction","title":"Introduction","text":"<p>LightDiffusion-Next is a cutting-edge tool designed to generate high-quality images from text prompts. This project aims to simplify and enhance the process of creating stunning visuals using advanced machine learning techniques.</p>"},{"location":"CONTRIBUTING/#how-to-contribute","title":"How to Contribute","text":"<p>There are several ways you can contribute to LightDiffusion-Next:</p> <ul> <li>Reporting Bugs: If you find a bug, please report it by opening an issue.</li> <li>Suggesting Enhancements: If you have an idea for a new feature or an improvement, feel free to suggest it.</li> <li>Submitting Pull Requests: If you have a fix or a new feature, you can submit a pull request.</li> <li>Improving Documentation: Help us improve our documentation by suggesting changes or adding new content.</li> </ul>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>Please read and follow our Code of Conduct to ensure a welcoming and inclusive environment for everyone.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>To get started with contributing, follow these steps:</p> <ol> <li>Fork the Repository: Click the \u201cFork\u201d button at the top right of the repository page.</li> <li>Clone Your Fork: Clone your forked repository to your local machine.     <code>bash     git clone https://github.com/your-username/LightDiffusion.git     cd LightDiffusion</code></li> <li>Create a Branch: Create a new branch for your changes.     <code>bash     git checkout -b my-feature-branch</code></li> <li>Make Changes: Make your changes to the codebase.</li> <li>Commit Changes: Commit your changes with a descriptive commit message.     <code>bash     git commit -m \"Add new feature\"</code></li> <li>Push Changes: Push your changes to your forked repository.     <code>bash     git push origin my-feature-branch</code></li> <li>Open a Pull Request: Open a pull request to the main repository.</li> </ol>"},{"location":"CONTRIBUTING/#reporting-issues","title":"Reporting Issues","text":"<p>If you encounter any issues, please report them by opening an issue on the GitHub repository. Provide as much detail as possible to help us understand and resolve the issue.</p>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting Changes","text":"<p>When submitting changes, please ensure that you:</p> <ul> <li>Follow the project\u2019s coding standards and style guide.</li> <li>Write clear and concise commit messages.</li> <li>Include tests for any new features or bug fixes.</li> <li>Update the documentation if necessary.</li> </ul>"},{"location":"CONTRIBUTING/#style-guide","title":"Style Guide","text":"<p>To maintain consistency in the codebase, please follow these guidelines:</p> <ul> <li>Use meaningful variable and function names.</li> <li>Write comments to explain complex logic.</li> <li>Follow the PEP 8 style guide for Python code.</li> <li>Ensure your code is properly formatted and linted.</li> </ul>"},{"location":"CONTRIBUTING/#community-and-support","title":"Community and Support","text":"<p>Join our community on GitHub to share your creations, report issues, and contribute to the project. Check out the FAQ for common questions and troubleshooting tips.</p> <p>Thank you for contributing to LightDiffusion! Your support and contributions are greatly appreciated.</p>"},{"location":"DiscordBot/","title":"DiscordBot Installation Guide for LightDiffusion","text":"<p>Welcome to the Discord-Bot installation guide for LightDiffusion. Follow the steps below to set up the Discord-Bot on your system.</p>"},{"location":"DiscordBot/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.10.6</li> <li>Git</li> <li><code>py-cord</code> library</li> <li>A Discord bot token (stored in a <code>.env</code> file)</li> </ul>"},{"location":"DiscordBot/#installation-steps","title":"Installation Steps","text":""},{"location":"DiscordBot/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>Open your terminal and run the following command to clone the repository inside the LightDiffusion-Next directory:</p> <pre><code>cd /path/to/LightDiffusion\ngit clone https://github.com/Aatrick/Boubou.git\ncd Boubou\n</code></pre>"},{"location":"DiscordBot/#2-install-dependencies","title":"2. Install Dependencies","text":"<p>With the LightDiffusion-Next venv activated, install the required Python dependencies using <code>pip</code>:</p> <pre><code>pip install -r requirements.txt\npip install py-cord\n</code></pre>"},{"location":"DiscordBot/#3-set-up-the-environment-variables","title":"3. Set Up the Environment Variables","text":"<p>Create a <code>.env</code> file in the <code>Boubou</code> directory and add your Discord bot token:</p> <pre><code>TOKEN = your_discord_bot_token_here\n</code></pre>"},{"location":"DiscordBot/#4-run-the-bot","title":"4. Run the Bot","text":"<p>Execute the following command to start the Discord bot:</p> <pre><code>python bot.py\n</code></pre> <p></p>"},{"location":"DiscordBot/#tips-and-tricks","title":"Tips and Tricks","text":"<p>[!TIP] - Ensure your Discord bot has the necessary permissions to read and send messages in your server. - For best performance, run the bot on a server with a stable internet connection. - Refer to the Discord Developer Portal for more information on creating and managing your Discord bot.</p>"},{"location":"DiscordBot/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation or usage, please refer to the FAQ or open an issue on the GitHub repository.</p> <p>Wish you good generations!</p>"},{"location":"Flux/","title":"Flux Installation Guide for LightDiffusion-Next","text":"<p>Welcome to the Flux installation guide for LightDiffusion-Next. Follow the steps below to set up Flux on your system.</p>"},{"location":"Flux/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Prerequisites</li> <li>Run the Application</li> <li>Using Flux</li> <li>Tips and Tricks</li> <li>Troubleshooting</li> </ol>"},{"location":"Flux/#introduction","title":"Introduction","text":"<p>Flux is a powerful AI model compatible with LightDiffusion-Next that offers enhanced image generation capabilities. This guide will walk you through the process of setting up Flux to work with LightDiffusion-Next.</p>"},{"location":"Flux/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>At least 25GB of free space on your hard drive (40-50GB recommended)</li> <li>A CUDA-compatible GPU with at least 6GB VRAM (16GB+ recommended)</li> </ul>"},{"location":"Flux/#run-the-application","title":"Run the Application","text":""},{"location":"Flux/#windows","title":"Windows","text":"<p>Open a command prompt and execute the <code>run.bat</code> file to start the application:</p> <pre><code>./run.bat\n</code></pre>"},{"location":"Flux/#linux","title":"Linux","text":"<p>Open a terminal and run the <code>run.sh</code> script to launch the application:</p> <pre><code>chmod +x run.sh\n./run.sh\n</code></pre>"},{"location":"Flux/#using-flux","title":"Using Flux","text":"<p>To use Flux with LightDiffusion-Next:</p>"},{"location":"Flux/#via-gui","title":"Via GUI","text":"<ol> <li>Launch LightDiffusion-Next.</li> <li>Select the Flux model from the dropdown menu.</li> <li>Configure your generation parameters.</li> <li>Click \u201cGenerate\u201d to create images using Flux.</li> </ol>"},{"location":"Flux/#via-cli","title":"Via CLI","text":"<p>Use the following command to generate images with Flux:</p> <pre><code>./pipeline.bat \"your prompt here\" width height number_of_images batch_size --flux\n</code></pre> <p>For example:</p> <pre><code>./pipeline.bat \"A beautiful sunset over the ocean\" 1024 768 1 1 --flux\n</code></pre>"},{"location":"Flux/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>Recommended Resolutions: Flux works best with specific resolutions. Refer to this guide for optimal resolution choices.</li> <li>Memory Management: Flux requires significant VRAM. Adjust batch sizes and resolution settings to optimize memory usage.</li> <li>Experiment with Prompts: Flux may respond differently to prompts compared to standard models. Experiment with prompt styles to find what works best.</li> </ul>"},{"location":"Flux/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues while using Flux, consider the following:</p> <ul> <li>Check VRAM: Ensure you have enough VRAM available. Flux models typically require more memory than standard models.</li> <li>Update Drivers: Make sure your GPU drivers are up to date to avoid compatibility issues.</li> <li>Reduce Resolution: If you\u2019re running out of memory, try reducing the resolution or batch size.</li> </ul> <p>For additional help, please refer to the GitHub issues page or join the community discussion forum.</p> <p>Wish you good generations!</p>"},{"location":"HiresFix%20%26%20Adetailer/","title":"HiresFix and Adetailer Guide for LightDiffusion-Next","text":"<p>Welcome to the HiresFix and Adetailer guide for LightDiffusion-Next. This document will help you understand how to use these features to enhance your generated images.</p>"},{"location":"HiresFix%20%26%20Adetailer/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>HiresFix    - Overview    - Usage in GUI    - Usage in CLI    - Tips and Tricks</li> <li>Adetailer    - Overview    - Usage in GUI    - Usage in CLI    - Tips and Tricks</li> <li>Using with Other Features</li> <li>Troubleshooting</li> </ol>"},{"location":"HiresFix%20%26%20Adetailer/#introduction","title":"Introduction","text":"<p>LightDiffusion-Next offers powerful tools like HiresFix and Adetailer to improve the quality and details of your generated images. These features are available in both the GUI and command-line interfaces, allowing you to achieve higher resolution and more refined details in your outputs.</p>"},{"location":"HiresFix%20%26%20Adetailer/#hiresfix","title":"HiresFix","text":""},{"location":"HiresFix%20%26%20Adetailer/#overview","title":"Overview","text":"<p>HiresFix is a feature that enhances the resolution of your generated images. It uses advanced upscaling algorithms to increase image resolution while preserving and refining details. This technique is particularly useful when you want to create high-resolution images with sharp details.</p>"},{"location":"HiresFix%20%26%20Adetailer/#usage-in-gui","title":"Usage in GUI","text":"<p>To use HiresFix in the GUI:</p> <ol> <li>Launch LightDiffusion-Next: Start the application using <code>run.bat</code> (Windows) or <code>run.sh</code> (Linux).</li> <li>Enable HiresFix: In the LightDiffusion-Next GUI, check the \u201cHiresFix\u201d checkbox.</li> <li>Configure Settings: Set your desired width, height, and other generation parameters.</li> <li>Generate Image: Click the \u201cGenerate\u201d button to create your image with HiresFix applied.</li> </ol>"},{"location":"HiresFix%20%26%20Adetailer/#usage-in-cli","title":"Usage in CLI","text":"<p>To use HiresFix in the command-line interface:</p> <pre><code>./pipeline.bat \"your prompt here\" width height number_of_images batch_size --hires-fix\n</code></pre> <p>For example:</p>"},{"location":"HiresFix%20%26%20Adetailer/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>Balance Resolution and Performance: Higher resolutions require more VRAM and processing time. Find a balance that works with your hardware.</li> <li>Combine with Stable-Fast: Use the \u2013stable-fast flag with HiresFix for improved performance, especially on high-resolution images.</li> <li>Adjust Batch Size: When using HiresFix, consider reducing batch size to manage memory usage.</li> <li>Try Different Resolutions: Experiment with various width and height combinations to find the optimal resolution for your specific image.</li> </ul>"},{"location":"HiresFix%20%26%20Adetailer/#adetailer","title":"Adetailer","text":""},{"location":"HiresFix%20%26%20Adetailer/#overview_1","title":"Overview","text":"<p>Adetailer (Automatic Detailer) is a feature that enhances specific details in your generated images, particularly focusing on faces, eyes, and human features. It uses advanced detection models to identify and refine these elements, resulting in more realistic and polished outputs.</p>"},{"location":"HiresFix%20%26%20Adetailer/#usage-in-gui_1","title":"Usage in GUI","text":"<p>To use Adetailer in the GUI:</p> <ul> <li>Launch LightDiffusion-Next: Start the application using run.bat (Windows) or run.sh (Linux).</li> <li>Enable Adetailer: In the LightDiffusion-Next GUI, check the \u201cAdetailer\u201d checkbox.</li> <li>Configure Settings: Set your desired width, height, and other generation parameters.</li> <li>Generate Image: Click the \u201cGenerate\u201d button to create your image with Adetailer applied.</li> </ul>"},{"location":"HiresFix%20%26%20Adetailer/#usage-in-cli_1","title":"Usage in CLI","text":"<p>To use Adetailer in the command-line interface:</p> <pre><code>./pipeline.bat \"your prompt here\" width height number_of_images batch_size --adetailer\n</code></pre> <p>For example:</p> <pre><code>./pipeline.bat \"portrait of a woman with blue eyes\" 512 768 1 1 --adetailer\n</code></pre>"},{"location":"HiresFix%20%26%20Adetailer/#tips-and-tricks_1","title":"Tips and Tricks","text":"<ul> <li>Focus on Human Subjects: Adetailer works best with images containing human faces and bodies.</li> <li>Combine with HiresFix: For optimal results, use Adetailer together with HiresFix by enabling both options.</li> <li>Use Quality Prompts: Include detailed descriptions of faces and features in your prompts to guide Adetailer.</li> <li>Be Patient: Adetailer performs additional processing steps, which may increase generation time.</li> </ul>"},{"location":"HiresFix%20%26%20Adetailer/#using-with-other-features","title":"Using with Other Features","text":"<p>HiresFix and Adetailer can be combined with other LightDiffusion-Next features for even better results:</p>"},{"location":"HiresFix%20%26%20Adetailer/#with-stable-fast","title":"With Stable-Fast","text":"<pre><code>./pipeline.bat \"your prompt here\" width height number_of_images batch_size --hires-fix --adetailer --stable-fast\n</code></pre>"},{"location":"HiresFix%20%26%20Adetailer/#with-prompt-enhancement","title":"With Prompt Enhancement","text":"<pre><code>./pipeline.bat \"your prompt here\" width height number_of_images batch_size --hires-fix --adetailer --enhance-prompt\n</code></pre>"},{"location":"HiresFix%20%26%20Adetailer/#with-autohdr","title":"With AutoHDR","text":"<pre><code>./pipeline.bat \"your prompt here\" width height number_of_images batch_size --hires-fix --adetailer --autohdr\n</code></pre>"},{"location":"HiresFix%20%26%20Adetailer/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with HiresFix or Adetailer, consider the following:</p> <ul> <li>Check VRAM Usage: High-resolution images with HiresFix and Adetailer may require significant VRAM. If you\u2019re experiencing out-of-memory errors, try reducing image dimensions or batch size.</li> <li>Update Dependencies: Ensure you have the latest version of LightDiffusion-Next and all its dependencies.</li> <li>Check Log Files: Review the application logs for specific error messages. Try Different Models: Some models work better with HiresFix and Adetailer than others. Experiment with different checkpoint files.</li> <li>Simplify Your Workflow: If you\u2019re experiencing issues, try running with fewer features enabled to isolate the problem. Refer to the FAQ: Check the FAQ for common issues and solutions.</li> </ul> <p>Wish you good generations!</p>"},{"location":"Img2Img/","title":"Img2Img Guide for LightDiffusion","text":"<p>Welcome to the Img2Img guide for LightDiffusion. This document will help you understand how to use the Img2Img feature in LightDiffusion-Next to transform existing images into new, high-quality visuals.</p>"},{"location":"Img2Img/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Prerequisites</li> <li>Basic Usage<ul> <li>Using the GUI</li> <li>Using the CLI</li> </ul> </li> <li>Tips and Tricks</li> <li>Troubleshooting</li> </ol>"},{"location":"Img2Img/#introduction","title":"Introduction","text":"<p>Img2Img is a powerful feature in LightDiffusion-Next that allows you to generate new images based on existing ones. By providing a source image and a text prompt, you can create stunning transformations and enhancements.</p>"},{"location":"Img2Img/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>LightDiffusion-Next properly installed on your system.</li> <li>A source image that you want to transform.</li> <li>Basic understanding of how to create prompts in LightDiffusion. Refer to the Prompting Guide for more information.</li> </ul>"},{"location":"Img2Img/#basic-usage","title":"Basic Usage","text":""},{"location":"Img2Img/#using-the-gui","title":"Using the GUI","text":"<ol> <li>Launch LightDiffusion-Next: Start the application using <code>run.bat</code> (Windows) or <code>run.sh</code> (Linux).</li> <li>Set Parameters:<ul> <li>Enter your prompt describing the desired output</li> <li>Set the denoising strength (how much change will be applied)</li> <li>Configure other settings like width, height, and sampling method</li> </ul> </li> <li>Generate: Click the \u201cImg2Img\u201d button to select and transform your image.</li> </ol>"},{"location":"Img2Img/#using-the-cli","title":"Using the CLI","text":"<p>To use Img2Img in the command-line interface:</p> <pre><code>./pipeline.bat \"path/to/your/image.png\" width height number_of_images batch_size --img2img\n</code></pre> <p>For example:</p> <pre><code>./pipeline.bat \"./_internal/output/Classic/LD_00001_.png\" 1024 768 1 1 --img2img\n</code></pre>"},{"location":"Img2Img/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>Experiment with CFG Scale: The CFG scale controls the adherence to the prompt. Higher values result in more adherence but can lead to overfitting.</li> <li>Use High-Quality Descriptions: Detailed and specific descriptions yield better results.</li> <li>Leverage Negative Prompts: Use negative prompts to filter out unwanted elements.</li> <li>Iterate and Refine: Generate multiple images and refine your prompts based on the results.</li> </ul>"},{"location":"Img2Img/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with Img2Img, consider the following:</p> <ul> <li>Check for Typos: Ensure there are no typos in your prompt.</li> <li>Simplify Your Prompt: Start with a simple prompt and gradually add complexity.</li> <li>Open an Issue: If you encounter persistent issues, open an issue on the LightDiffusion-Next GitHub repository</li> </ul> <p>Wish you good generations!</p>"},{"location":"Installation/","title":"Installation Guide for LightDiffusion-Next","text":"<p>Welcome to the LightDiffusion-Next installation guide. Follow the steps below to set up LightDiffusion-Next on your system.</p>"},{"location":"Installation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Prerequisites</li> <li>Installation Steps    - Clone the Repository    - Run the Application    - Add Model Checkpoints</li> <li>Tips and Tricks</li> <li>Troubleshooting</li> </ol>"},{"location":"Installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.10.6</li> <li>Git</li> <li>At least 10GB of free space on your hard drive</li> </ul>"},{"location":"Installation/#installation-steps","title":"Installation Steps","text":""},{"location":"Installation/#clone-the-repository","title":"Clone the Repository","text":"<p>Open your terminal and run the following command to clone the repository:</p> <pre><code>git clone https://github.com/Aatrick/LightDiffusion-Next.git\ncd LightDiffusion-Next\n</code></pre>"},{"location":"Installation/#run-the-application","title":"Run the Application","text":""},{"location":"Installation/#windows","title":"Windows","text":"<p>Open a command prompt and execute the <code>run.bat</code> file to start the application:</p> <pre><code>./run.bat\n</code></pre>"},{"location":"Installation/#linux","title":"Linux","text":"<p>Open a terminal and execute the <code>run.sh</code> file to start the application:</p> <pre><code>./run.sh\n</code></pre>"},{"location":"Installation/#add-model-checkpoints","title":"Add Model Checkpoints","text":"<p>Download your SD1/1.5 safetensors model and place it in the checkpoints directory.</p>"},{"location":"Installation/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>Ensure your system meets the minimum requirements for optimal performance.</li> <li>For best results, use resolutions corresponding to megapixel sizes. Refer to this guide for more information.</li> <li>If you\u2019re using a laptop, make sure it\u2019s plugged in to avoid performance throttling.</li> </ul>"},{"location":"Installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation or usage, consider the following:</p> <ul> <li>Check Python Version: Ensure you\u2019re using Python 3.10.6 or later. Other versions may cause compatibility issues.</li> <li>CUDA Issues: Make sure you have the correct CUDA version installed for your GPU.</li> <li>Path Issues: Verify that Python is in your system PATH.</li> <li>Dependency Problems:  Try reinstalling dependencies with <code>pip install -r requirements.txt.</code></li> </ul> <p>If you\u2019re still facing issues, feel free to reach out to the LightDiffusion-Next community for assistance or open an issue on the GitHub repository</p> <p>Wish you good generations!</p>"},{"location":"LoRas/","title":"LoRas Guide for LightDiffusion","text":"<p>Welcome to the LoRas guide for LightDiffusion. This document will help you understand how to use LoRas (Low-Rank Adaptations) to enhance your image generation process.</p>"},{"location":"LoRas/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>What are LoRas?</li> <li>Loading LoRas</li> <li>Using LoRas</li> <li>Advanced Techniques</li> <li>Troubleshooting</li> <li>FAQ</li> </ol>"},{"location":"LoRas/#introduction","title":"Introduction","text":"<p>LightDiffusion-Next offers powerful tools like LoRas to improve the quality and details of your generated images. By leveraging these features, you can achieve higher resolution and more refined details in your outputs.</p>"},{"location":"LoRas/#what-are-loras","title":"What are LoRas?","text":"<p>LoRas (Low-Rank Adaptations) are a technique used to fine-tune pre-trained models with a smaller number of parameters. This allows for efficient adaptation of models to new tasks or domains without the need for extensive retraining.</p>"},{"location":"LoRas/#loading-loras","title":"Loading LoRas","text":"<p>To load LoRas in LightDiffusion, follow these steps:</p> <ol> <li> <p>Prepare Your LoRas: Ensure your LoRas files are in the correct format and located in the appropriate directory (<code>./_internal/loras</code>).</p> </li> <li> <p>Load LoRas in the GUI:</p> </li> </ol> <ul> <li>Open the LightDiffusion-Next GUI.</li> <li>Select the wanted LoRas from the dropdown menu.</li> </ul> <ol> <li>Verify Loaded LoRas: Check if the LoRas have been loaded correctly by inspecting the loaded keys and parameters.</li> </ol>"},{"location":"LoRas/#using-loras","title":"Using LoRas","text":"<p>Once loaded, you can use LoRas to enhance your image generation process. Here\u2019s how:</p> <ol> <li> <p>Enhance Prompts: Apply LoRas to enhance your prompts by selecting the desired LoRas in the GUI.</p> </li> <li> <p>Generate Images: Use the enhanced model to generate images with improved quality and details.</p> </li> </ol>"},{"location":"LoRas/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"LoRas/#combining-multiple-loras","title":"Combining Multiple LoRas","text":"<p>You can combine multiple LoRas to achieve more complex adaptations. Ensure that the LoRas are compatible and do not conflict with each other. To combine multiple LoRas, load them together and apply them sequentially to your model by modifying the execution pipeline at the bottom of the <code>LightDiffusion.py</code> file.</p>"},{"location":"LoRas/#fine-tuning-loras","title":"Fine-Tuning LoRas","text":"<p>For advanced users, fine-tuning LoRas can provide even better results. Experiment with different parameters and configurations to find the optimal settings for your tasks.</p>"},{"location":"LoRas/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with LoRas, consider the following:</p> <ul> <li>Check File Formats: Ensure your LoRas files are in the correct format (<code>.safetensors</code>).</li> <li>Verify Directory Paths: Confirm that the LoRas files are located in the correct directory (<code>./_internal/loras</code>).</li> <li>Inspect Logs: Check the logs for detailed information on what might be going wrong.</li> </ul>"},{"location":"LoRas/#faq","title":"FAQ","text":"<p>Q: What are the supported formats for LoRas?</p> <p>A: LightDiffusion-Next supports <code>.safetensors</code> format for LoRas.</p> <p>Q: How do I combine multiple LoRas?</p> <p>A: You can combine multiple LoRas by loading them together and applying them to your model.</p> <p>Q: Where can I find more information?</p> <p>A: For more information, refer to the LightDiffusion-Next documentation and the FAQ.</p> <p>Thank you for using LightDiffusion! We hope this guide helps you create amazing visuals with enhanced prompts.</p>"},{"location":"Ollama/","title":"Ollama Integration Guide for LightDiffusion-Next","text":"<p>Welcome to the Ollama integration guide for LightDiffusion-Next. This document will help you understand how to set up and use the Ollama prompt enhancer to improve your image generation prompts.</p>"},{"location":"Ollama/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Prerequisites</li> <li>Installation</li> <li>Usage<ul> <li>Using in GUI</li> <li>Using in CLI</li> </ul> </li> <li>Tips and Tricks</li> <li>Troubleshooting</li> <li>FAQ</li> </ol>"},{"location":"Ollama/#introduction","title":"Introduction","text":"<p>Ollama is a powerful tool that enhances your text prompts to generate higher quality images with LightDiffusion-Next. By integrating Ollama, you can leverage advanced language models to automatically refine and optimize your prompts for better results.</p>"},{"location":"Ollama/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>LightDiffusion-Next installed on your system</li> <li>Python 3.10.6 or later</li> <li>At least 4GB of free RAM for running language models</li> <li>Internet connection for the initial download</li> </ul>"},{"location":"Ollama/#installation","title":"Installation","text":"<p>To install the Ollama prompt enhancer, follow these steps:</p> <ol> <li> <p>Use the official installer</p> <ul> <li>Download the official installer from the Ollama website.</li> <li>Run the installer and follow the on-screen instructions.</li> </ul> </li> <li> <p>Verify installation</p> <ul> <li>Open a terminal or command prompt.</li> <li>Run the following command to check the installation:</li> </ul> <p><code>ollama --version</code></p> <p>You should see the version information if the installation was successful.</p> </li> </ol>"},{"location":"Ollama/#usage","title":"Usage","text":""},{"location":"Ollama/#using-in-gui","title":"Using in GUI","text":"<ol> <li>Launch LightDiffusion-Next Start the application using <code>run.bat</code> (Windows) or <code>run.sh</code> (Linux).</li> <li>Enable Prompt Enhancement Check the \u201cPrompt Enhancer\u201d checkbox in the LightDiffusion-Next interface.</li> <li>Enter a Base Prompt Type your initial prompt in the prompt field.</li> <li>Generate Images Click \u201cGenerate\u201d and Ollama will automatically enhance your prompt before image generation.</li> </ol>"},{"location":"Ollama/#using-in-cli","title":"Using in CLI","text":"<p>To use Ollama prompt enhancement in the command-line interface:</p> <p>For example:</p> <pre><code>./pipeline.bat \"your prompt here\" width height number_of_images batch_size --enhance-prompt\n</code></pre>"},{"location":"Ollama/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>Write Natural Prompts: Use conversational language and natural phrasing for better results.</li> <li>Review Enhanced Prompts: Examine how Ollama modifies your prompts to learn effective prompt patterns.</li> <li>Combine with LoRAs: Use Ollama-enhanced prompts with appropriate LoRAs for even better results.</li> <li>Iterate Gradually: If the enhanced result isn\u2019t what you expected, modify your base prompt slightly and try again.</li> <li>Balance Specificity: Be specific enough to guide the AI but leave room for the enhancer to add details.</li> </ul>"},{"location":"Ollama/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with the Ollama integration, try these steps:</p> <ul> <li>Check Ollama Service: Ensure the Ollama service is running on your system.</li> <li>Restart Integration: Sometimes simply restarting LightDiffusion-Next can resolve connection issues.</li> <li>Check Network: Make sure your firewall isn\u2019t blocking the connection between LightDiffusion-Next and Ollama.</li> <li>Update Models: Ensure you\u2019re using the latest version of the language models.</li> <li>Check Logs: Examine the application logs for specific error messages.</li> </ul>"},{"location":"Ollama/#faq","title":"FAQ","text":"<p>Q: Which language model works best with LightDiffusion-Next? A: We recommend starting with the \u201cdeepseek-r1\u201d model for a good balance of speed and quality.</p> <p>Q: How much will Ollama integration slow down my generation process? A: The prompt enhancement typically adds only a few seconds to the process, depending on your hardware and the complexity of the prompt.</p> <p>Q: Can I use Ollama offline? A: Yes, once the models are downloaded, Ollama can function without an internet connection.</p> <p>Q: How can I see what Ollama did to my prompt? A: Enhanced prompts are displayed in the CLI before generation.</p> <p>Wish you good generations!</p>"},{"location":"Prompting/","title":"Prompting Guide for LightDiffusion","text":"<p>Welcome to the LightDiffusion-Next Prompting Guide. This document will help you understand how to create effective prompts for generating high-quality images using LightDiffusion.</p>"},{"location":"Prompting/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Basic Prompt Structure</li> <li>Advanced Prompt Techniques<ul> <li>Using Parentheses for Emphasis</li> <li>Using Colons for Weight</li> <li>Combining Multiple Concepts</li> </ul> </li> <li>Negative Prompts</li> <li>Prompt Examples</li> <li>Tips and Tricks</li> <li>Troubleshooting</li> </ol>"},{"location":"Prompting/#introduction","title":"Introduction","text":"<p>LightDiffusion-Next is a powerful tool for generating images from text prompts. By understanding how to structure your prompts, you can create stunning visuals with ease.</p>"},{"location":"Prompting/#basic-prompt-structure","title":"Basic Prompt Structure","text":"<p>A basic prompt in LightDiffusion-Next consists of a description of the desired image. Here is the general structure of the <code>prompt.txt</code> file :</p> <pre><code>prompt: &lt;description of the image&gt;\nneg: &lt;negative prompts&gt;\nw: &lt;width&gt;\nh: &lt;height&gt;\ncfg: &lt;cfg scale&gt;\n</code></pre>"},{"location":"Prompting/#example","title":"Example","text":"<pre><code>prompt: a beautiful sunset over the mountains, vibrant colors, high resolution\nneg: low quality, blurry\nw: 1024\nh: 768\ncfg: 7\n</code></pre>"},{"location":"Prompting/#advanced-prompt-techniques","title":"Advanced Prompt Techniques","text":""},{"location":"Prompting/#using-parentheses-for-emphasis","title":"Using Parentheses for Emphasis","text":"<p>You can use parentheses to emphasize certain parts of your prompt. The more parentheses you use, the more emphasis is placed on that part of the prompt.</p> <pre><code>prompt: ((a beautiful sunset)), over the mountains, (vibrant colors), high resolution\n</code></pre>"},{"location":"Prompting/#using-colons-for-weight","title":"Using Colons for Weight","text":"<p>You can use colons to increase the weight of certain parts of your prompt. The higher the weight, the more importance is given to that part of the prompt.</p> <pre><code>prompt: (a beautiful sunset:1.2), over the mountains, vibrant colors, (high resolution:1.5)\n</code></pre>"},{"location":"Prompting/#combining-multiple-concepts","title":"Combining Multiple Concepts","text":"<p>You can combine multiple concepts in a single prompt to create more complex images.</p> <pre><code>prompt: a futuristic cityscape, neon lights, flying cars, (cyberpunk style)\n</code></pre> <p>[!TIP] Use High-Quality Descriptions: Detailed and specific descriptions yield better results.</p>"},{"location":"Prompting/#negative-prompts","title":"Negative Prompts","text":"<p>Negative prompts help you avoid unwanted elements in your generated images.</p> <pre><code>neg: low quality, blurry, distorted\n</code></pre> <p>[!TIP] Experiment with CFG Scale: The CFG scale controls the adherence to the prompt. Higher values result in more adherence but can lead to overfitting.</p>"},{"location":"Prompting/#quality-tokens","title":"Quality tokens","text":"<p>Being trained on image datasets, you can use quality tokens/keywords to improve the quality of the generated images.</p> <pre><code>masterpiece, ****,  best quality, (extremely detailed CG unity 8k wallpaper, ultra-detailed, best shadow), (beautiful detailed face, beautiful detailed eyes), High contrast, ((colourful paint splashes on transparent background, dulux,)), ((caustic)), dynamic angle, beautiful detailed glow, wallpaper, (detailed background), (best illumination, an extremely delicate and beautiful), hyper detail, intricate details,\n</code></pre>"},{"location":"Prompting/#embeddings","title":"Embeddings","text":"<p>You can use embeddings in the negative prompts to have premade negative prompts, reducing the need to write them out.</p> <pre><code>neg: (embedding:EasyNegative), (embedding:badhandv4:1.2)\n</code></pre>"},{"location":"Prompting/#camera-angles","title":"Camera Angles","text":"<p>You can specify the camera angle in your prompt to control the perspective of the generated image.</p> <pre><code>dynamic angle, top-down view, bird's eye view, close-up shot, wide-angle shot, pov, third-person perspective, panoramic view, cowboy shot, full body, looking at viewer, ...\n</code></pre>"},{"location":"Prompting/#lighting","title":"Lighting","text":"<p>You can specify the lighting conditions in your prompt to achieve the desired effect.</p> <pre><code>best illumination, cinematic light, dramatic light, beautiful glow, chromatic aberration, high contrast, dynamic lighting, soft shadows, warm light, cool light, natural light, ...\n</code></pre>"},{"location":"Prompting/#flux","title":"Flux","text":"<p>Prompting for Flux is a bit different than LightDiffusion, since you don\u2019t have a negative prompt. You should start your prompt with a phrase in natural language, and then add the quality tokens and other details.</p> <pre><code>prompt: a beautiful sunset over the mountains, vibrant colors, high resolution, (best quality, ultra-detailed, high contrast)\n</code></pre>"},{"location":"Prompting/#prompt-examples","title":"Prompt Examples","text":""},{"location":"Prompting/#simple-prompt","title":"Simple Prompt","text":"<pre><code>prompt: a serene beach at sunrise\nneg: low quality, dark\nw: 800\nh: 600\ncfg: 6\n</code></pre>"},{"location":"Prompting/#detailed-prompt","title":"Detailed Prompt","text":"<pre><code>prompt: (a majestic dragon), flying over a medieval castle, (high detail), (fantasy art)\nneg: cartoonish, low quality\nw: 1024\nh: 1024\ncfg: 8\n</code></pre>"},{"location":"Prompting/#artistic-style","title":"Artistic Style","text":"<pre><code>prompt: a portrait of a woman, (in the style of Van Gogh), (impressionist painting)\nneg: low quality, abstract\nw: 768\nh: 1024\ncfg: 7\n</code></pre>"},{"location":"Prompting/#advanced","title":"Advanced","text":"<pre><code>prompt: masterpiece, best quality, (extremely detailed CG unity 8k wallpaper, masterpiece, best quality, ultra-detailed, best shadow), (detailed background), (beautiful detailed face, beautiful detailed eyes), High contrast, (best illumination, an extremely delicate and beautiful),1girl,((colourful paint splashes on transparent background, dulux,)), ((caustic)), dynamic angle,beautiful detailed glow,full body, cowboy shot, colorful, 1girl, white hair, purple eyes, dual wielding, sword, holding sword, blue flames, glow, glowing weapon, light particles, wallpaper, chromatic aberration, (detailed background,dark fantasy), (beautiful detailed face), high contrast, (best illumination, an extremely delicate and beautiful), ((cinematic light)), colorful, hyper detail, dramatic light, intricate details, depth of field,black light particles,(broken glass),magic circle\n</code></pre>"},{"location":"Prompting/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>Start Simple: Begin with basic prompts and gradually add complexity.</li> <li>Be Specific: Detailed descriptions yield better results than vague ones.</li> <li>Use Quality Descriptors: Include terms like \u201chigh resolution\u201d, \u201cdetailed\u201d, \u201cmasterpiece\u201d, etc.</li> <li>Balance Positive and Negative Prompts: Use negative prompts to filter out unwanted elements.</li> <li>Experiment with Embeddings: Embeddings can save time and improve prompt quality.</li> <li>Experiment with Weight and Emphasis: Use colons and parentheses to adjust the importance of different parts of your prompt.</li> <li>Save Successful Prompts: Keep a record of prompts that generate good results for future reference.</li> </ul>"},{"location":"Prompting/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with your prompts, consider the following:</p> <ul> <li>Simplify Your Prompt: Too much complexity can confuse the model.</li> <li>Check for Contradictions: Make sure your prompt doesn\u2019t contain conflicting instructions.</li> <li>Adjust CFG Scale: Try different values to find the right balance between creativity and adherence to the prompt.</li> <li>Try Different Models: Some models may perform better with certain types of prompts.</li> <li>Revise Negative Prompts: Make sure your negative prompts aren\u2019t too restrictive.</li> </ul> <p>For more assistance, refer to the community forums or the project\u2019s GitHub discussions.</p> <p>Wish you good generations!</p>"},{"location":"Stable-fast/","title":"Stable-fast","text":"<p>Royalty of the Stable-fast.md</p>"},{"location":"Stable-fast/#stable-fast-guide-for-lightdiffusion-next","title":"Stable-Fast Guide for LightDiffusion-Next","text":"<p>Welcome to the Stable-Fast guide for LightDiffusion-Next. This document will help you understand how to use Stable-Fast to accelerate your image generation process.</p>"},{"location":"Stable-fast/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Prerequisites</li> <li>Installation</li> <li>How to Enable Stable-Fast<ul> <li>In the GUI</li> <li>In the CLI</li> </ul> </li> <li>Technical Overview</li> <li>Performance Benefits</li> <li>Compatibility Notes</li> <li>Tips and Tricks</li> <li>Troubleshooting</li> </ol>"},{"location":"Stable-fast/#introduction","title":"Introduction","text":""},{"location":"Stable-fast/#what-is-this","title":"What is this?","text":"<p><code>stable-fast</code> is an ultra lightweight inference optimization framework for HuggingFace Diffusers on NVIDIA GPUs. <code>stable-fast</code> provides super fast inference optimization by utilizing some key techniques and features:</p>"},{"location":"Stable-fast/#installation","title":"Installation","text":"<p>NOTE: <code>stable-fast</code> is currently only tested on <code>Linux</code> and <code>WSL2 in Windows</code> on <code>Nvidia GPUs</code>.</p> <p>Running the <code>run.sh</code> or the <code>pipeline.sh</code>  scripts should work on most systems. If you encounter any issues, please refer to the command below:</p> <pre><code>pip install --index-url \"https://download.pytorch.org/whl/cu121\" \\\n        torch==2.2.2 torchvision \"xformers&gt;=0.0.22\" \"triton&gt;=2.1.0\" \\\n        stable_fast-1.0.5+torch222cu121-cp310-cp310-manylinux2014_x86_64.whl\n</code></pre>"},{"location":"Stable-fast/#how-to-enable-stable-fast","title":"How to Enable Stable-Fast","text":""},{"location":"Stable-fast/#in-the-gui","title":"In the GUI","text":"<ol> <li>Launch LightDiffusion-Next: Start the application using <code>run.bat</code> (Windows) or <code>run.sh</code> (Linux).</li> <li>Enable Stable-Fast: Check the \u201cStable-Fast\u201d checkbox in the LightDiffusion-Next interface.</li> <li>Configure Settings: Set your desired parameters for image generation.</li> <li>Generate Images: Click the \u201cGenerate\u201d button to create images with accelerated processing.</li> </ol>"},{"location":"Stable-fast/#in-the-cli","title":"In the CLI","text":"<p>To enable Stable-Fast in the command-line interface, use the following command:</p> <pre><code>./pipeline.bat \"your prompt here\" width height number_of_images batch_size --stable-fast\n</code></pre> <p>For example:</p> <pre><code>./pipeline.sh \"A beautiful sunset over the ocean\" 1024 768 1 1 --stable-fast\n</code></pre>"},{"location":"Stable-fast/#technical-overview","title":"Technical Overview","text":"<ul> <li>CUDNN Convolution Fusion: <code>stable-fast</code> implements a series of fully-functional and fully-compatible CUDNN convolution fusion operators for all kinds of combinations of <code>Conv + Bias + Add + Act</code> computation patterns.</li> <li>Low Precision &amp; Fused GEMM: <code>stable-fast</code> implements a series of fused GEMM operators that compute with <code>fp16</code> precision, which is fast than PyTorch\u2019s defaults (read &amp; write with <code>fp16</code> while compute with <code>fp32</code>).</li> <li>Fused Linear GEGLU: <code>stable-fast</code> is able to fuse <code>GEGLU(x, W, V, b, c) = GELU(xW + b) \u2297 (xV + c)</code> into one CUDA kernel.</li> <li>NHWC &amp; Fused GroupNorm: <code>stable-fast</code> implements a highly optimized fused NHWC <code>GroupNorm + Silu</code> operator with OpenAI\u2019s <code>Triton</code>, which eliminates the need of memory format permutation operators.</li> <li>Fully Traced Model: <code>stable-fast</code> improves the <code>torch.jit.trace</code> interface to make it more proper for tracing complex models. Nearly every part of <code>StableDiffusionPipeline/StableVideoDiffusionPipeline</code> can be traced and converted to TorchScript. It is more stable than <code>torch.compile</code> and has a significantly lower CPU overhead than <code>torch.compile</code> and supports ControlNet and LoRA.</li> <li>CUDA Graph: <code>stable-fast</code> can capture the <code>UNet</code>, <code>VAE</code> and <code>TextEncoder</code> into CUDA Graph format, which can reduce the CPU overhead when the batch size is small. This implemention also supports dynamic shape.</li> <li>Fused Multihead Attention: <code>stable-fast</code> just uses xformers and makes it compatible with TorchScript.</li> </ul> <p>My next goal is to keep <code>stable-fast</code> as one of the fastest inference optimization frameworks for <code>diffusers</code> and also provide both speedup and VRAM reduction for <code>transformers</code>. In fact, I already use <code>stable-fast</code> to optimize LLMs and achieve a significant speedup. But I still need to do some work to make it more stable and easy to use and provide a stable user interface.</p>"},{"location":"Stable-fast/#differences-with-other-acceleration-libraries","title":"Differences With Other Acceleration Libraries","text":"<ul> <li>Fast: <code>stable-fast</code> is specialy optimized for HuggingFace Diffusers. It achieves a high performance across many libraries. And it provides a very fast compilation speed within only a few seconds. It is significantly faster than <code>torch.compile</code>, <code>TensorRT</code> and <code>AITemplate</code> in compilation time.</li> <li>Minimal: <code>stable-fast</code> works as a plugin framework for <code>PyTorch</code>. It utilizes existing <code>PyTorch</code> functionality and infrastructures and is compatible with other acceleration techniques, as well as popular fine-tuning techniques and deployment solutions.</li> <li>Maximum Compatibility: <code>stable-fast</code> is compatible with all kinds of <code>HuggingFace Diffusers</code> and <code>PyTorch</code> versions. It is also compatible with <code>ControlNet</code> and <code>LoRA</code>. And it even supports the latest <code>StableVideoDiffusionPipeline</code> out of the box!</li> </ul>"},{"location":"Stable-fast/#performance-benefits","title":"Performance Benefits","text":"<p>Using Stable-Fast can provide:</p> <ul> <li>Up to 70% speedup in generation time</li> <li>Reduced memory usage</li> <li>Lower CPU overhead compared to <code>torch.compile</code></li> <li>Improved batch processing efficiency</li> </ul>"},{"location":"Stable-fast/#compatibility-notes","title":"Compatibility Notes","text":"<p>Stable-Fast is compatible with:</p> <ul> <li>Most SD1.x and SD2.x models</li> <li>ControlNet extensions</li> <li>LoRA adapters</li> <li>Various sampling methods</li> </ul> <p>It may have limited compatibility with some specialized or custom models.</p>"},{"location":"Stable-fast/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>First Run Warmup: The first generation with Stable-Fast may take slightly longer due to compilation overhead. Subsequent generations will be faster.</li> <li>Resolution Impact: The performance gains from Stable-Fast are more pronounced at higher resolutions.</li> <li>Combine with HiresFix: Stable-Fast works particularly well with HiresFix, allowing for higher resolution outputs with less performance penalty.</li> <li>Memory Management: While Stable-Fast reduces memory usage, it\u2019s still recommended to close other applications when generating at high resolutions.</li> <li>Testing: Run comparison tests with and without Stable-Fast to find the optimal configuration for your specific hardware.</li> </ul>"},{"location":"Stable-fast/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with Stable-Fast, try these solutions:</p> <ul> <li>Driver Updates: Ensure your NVIDIA drivers are up to date.</li> <li>CUDA Toolkit: Verify your CUDA toolkit installation.</li> <li>Model Compatibility: Some models may not be fully compatible with all Stable-Fast optimizations. Try a different model.</li> </ul> <p>For additional help, please refer to the GitHub repository or raise an issue on the LightDiffusion-Next issues page.</p> <p>Wish you good generations! ```</p>"}]}